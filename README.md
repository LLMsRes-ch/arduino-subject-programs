# Codes for the Paper: "Evaluating Large Language Models (LLMs) for Arduino Code Generation"

## Overview

Large Language Models (LLMs), also known as Generative AI, have revolutionized code generation by converting natural-language prompts into executable code. However, their capabilities in generating code for **resource-constrained devices** like **Arduino**, which are widely used in **Internet of Things (IoT)** and **embedded systems**, remain underexplored.

This repository contains a **set of 31 Arduino subject programs**, each provided with **two optimized reference solutions**, designed to evaluate the effectiveness of LLMs in generating correct, efficient, and high-quality Arduino code. These subject programs support empirical research on LLM performance in microcontroller-based programming environments.

---

## Subject Program Design

- ğŸ§© **Number of Programs**: 31 tasks, each with 2 optimized solutions
- ğŸ§  **Task Coverage**: Data types, control structures, functions, arrays, loops, sensors, and more  
- ğŸ“Ÿ **Platform**: Arduino Uno Rev3  
- ğŸ” **Optimization Focus**: Execution speed, memory usage, and code readability

Each program task was developed based on examples from the official Arduino documentation. Two solutions per task were created by the authors to reflect **developer variation** and different optimization strategies. Some solutions favor **memory efficiency**, while others prioritize **faster execution**.

---

## Purpose of the Study

These subject programs were used in a study that evaluated **six state-of-the-art LLMs** across five key dimensions:

1. âœ… **Functional Correctness**  
2. â±ï¸ **Runtime Efficiency**  
3. ğŸ’¾ **Memory & SRAM Usage**  
4. ğŸ§‘â€ğŸ’» **Code Complexity and Similarity**  
5. ğŸ” **Multi-Round Error Correction**

### Key Findings:
- **ChatGPT-4** achieved the highest zero-shot functional correctness and produced code most similar to human-written code.
- **Gemini 2.0 Flash** generated faster code but with higher complexity and lower similarity.
- **DeepSeek-V3** showed strong memory efficiency.
- **Claude 3.5 Sonnet** struggled with prompt interpretation.
- Multi-round prompting improved output correctness across all models.

---

## Prompt Structure

Each program is paired with a **zero-shot prompt** following a consistent and model-friendly structure.

